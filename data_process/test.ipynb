{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 111] Connection\n",
      "[nltk_data]     refused>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "predictor=Predictor.from_path(\"../model/structured-prediction-srl-bert.2020.12.15.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity():\n",
    "    def __init__(self,text,type,start,end) -> None:\n",
    "        self.text = text\n",
    "        self.type = type\n",
    "        self.start_offset=start\n",
    "        self.end_offset = end\n",
    "def Get_arguments(tokens,srl_dict,event_start_offset,event_end_offset,entities):\n",
    "    idx_mapping = {}\n",
    "    i,j = 0,0\n",
    "    srl_tokens = srl_dict['words']\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        while True:\n",
    "            if token.startswith(srl_tokens[j]):\n",
    "                start_idx = j\n",
    "                token_srl = srl_tokens[j]\n",
    "                j += 1\n",
    "                break\n",
    "            else:\n",
    "                j += 1\n",
    "        while token != token_srl and j < len(srl_tokens):\n",
    "            if token.startswith((token_srl+srl_tokens[j])):\n",
    "                token_srl += srl_tokens[j]\n",
    "            j += 1\n",
    "        idx_mapping[i] = (start_idx, j)#空格分的token与预测器分的token的对应关系\n",
    "        i += 1\n",
    "    arg0,arg1,loc,time=(-1,-1),(-1,-1),(-1,-1),(-1,-1),\n",
    "\n",
    "    #这里是ecb的逻辑，触发词作为事件mention,但是wec应该怎么标，只是一个事件mention，参数信息大多并不是描述这个时间的\n",
    "    for verb in srl_dict['verbs']:\n",
    "        flag = False\n",
    "        for i in range(idx_mapping[event_start_offset][0], idx_mapping[event_end_offset][1]):\n",
    "\n",
    "            if '-V' in verb['tags'][i]:  #\n",
    "                flag = True\n",
    "                break\n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        for entity in entities:\n",
    "            for i in range(idx_mapping[entity.start_offset][0], idx_mapping[entity.end_offset][1]):\n",
    "                if verb['tags'][i] != 'O':\n",
    "                    type = verb['tags'][i].split('-')[-1]\n",
    "                    if type == 'ARG0' and 'GPE' not in entity.type and 'DATE' not in entity.type:\n",
    "                        arg0 = (entity.start_offset, entity.end_offset)\n",
    "                        break\n",
    "                    if type == 'ARG1' and 'GPE' not in entity.type and 'DATE' not in entity.type:\n",
    "                        arg1 = (entity.start_offset, entity.end_offset)\n",
    "                        break\n",
    "                    if type == 'GPE' and 'GPE' in entity.type:\n",
    "                        loc = (entity.start_offset, entity.end_offset)\n",
    "                        break\n",
    "                    if type == 'TMP' and 'DATE' in entity.type:\n",
    "                        time = (entity.start_offset, entity.end_offset)\n",
    "                        break\n",
    "    return arg0,arg1,loc,time\n",
    "def Get_entities(doc,tokens):\n",
    "    i=0\n",
    "    entities=[]\n",
    "    for entity in doc.ents:#entity是按顺序给出的\n",
    "        start,end=-1,-1\n",
    "        flag = True\n",
    "        while(i<len(tokens) and flag):\n",
    "            tokens[i]\n",
    "            if entity.text.startswith(tokens[i]) or tokens[i].startswith(entity.text.split(' ')[-1]):\n",
    "                start = i\n",
    "                # palo alto ->palo alto-based\n",
    "                while True:\n",
    "                    if entity.text.endswith(tokens[i]) or tokens[i].startswith(entity.text.split(' ')[-1]):\n",
    "                        end = i\n",
    "                        flag=False#标记当前entity已找到\n",
    "                        break\n",
    "                    i+=1\n",
    "            i+=1\n",
    "        entities.append(Entity(entity.text,entity.label_,start,end)) #不能用这个start，end 并不是按空格划分的\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears to be a very daunting deal , '' said American Technology Research analyst Shaw Wu .\t11\t11\t16\t17\t-1\t-1\t-1\t-1\t-1\t-1\t\n",
    "# Palo Alto-based HP and Plano , Texas-based EDS confirmed the talks yesterday shortly after The Wall Street Journal reported a deal could be reached as early as today .\t23\t23\t-1\t-1\t-1\t-1\t-1\t-1\t27\t27\t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \" '' It appears to be a very daunting deal , '' said American Technology Research analyst Shaw Wu .\"\n",
    "text2 = \"Palo Alto-based HP and Plano ,yestarday Seven in the morning in India,Texas-based EDS confirmed the talks yesterday shortly after The Wall Street Journal reported a deal could be reached as early as today .\"\n",
    "tokens1 = text1.strip().split(\" \")\n",
    "tokens2 = text2.strip().split(\" \")\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "entities1=Get_entities(doc1,tokens1)\n",
    "entities2=Get_entities(doc2,tokens2)\n",
    "# Palo Alto-based 标注的是palo alto  但我应该在这个将实体定义为 palo alto-based，这样在tokenizer时也能拼起来\n",
    "\n",
    "src1_result = predictor.predict(sentence=text1)\n",
    "src2_result = predictor.predict(sentence=text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 17) (-1, -1) (-1, -1) (-1, -1)\n",
      "(-1, -1) (-1, -1) (-1, -1) (19, 27)\n"
     ]
    }
   ],
   "source": [
    "text1_arg0,text1_arg1,text1_loc,text1_time =Get_arguments(tokens1,src1_result,11,11,entities1)\n",
    "print(text1_arg0,text1_arg1,text1_loc,text1_time)\n",
    "text2_arg0,text2_arg1,text2_loc,text2_time =Get_arguments(tokens2,src2_result,23,23,entities2)\n",
    "print(text2_arg0,text2_arg1,text2_loc,text2_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(src1_result,src2_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palo Alto GPE 0 1\n",
      "HP and Plano ORG 2 4\n",
      "Texas GPE 6 6\n",
      "EDS ORG 7 7\n",
      "yesterday DATE 11 11\n",
      "The Wall Street Journal ORG 14 17\n",
      "as early as today DATE 19 27\n"
     ]
    }
   ],
   "source": [
    "for entity in entities2:\n",
    "    print(entity.text,entity.type,entity.start_offset,entity.end_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palo Alto GPE 0 1\n",
      "HP and Plano ORG 2 4\n",
      "Seven in the morning TIME 6 9\n",
      "India GPE 11 11\n",
      "Texas GPE -1 -1\n",
      "EDS ORG -1 -1\n",
      "yesterday DATE -1 -1\n",
      "The Wall Street Journal ORG -1 -1\n",
      "as early as today DATE -1 -1\n"
     ]
    }
   ],
   "source": [
    "for entity in entities2:\n",
    "    print(entity.text,entity.type,entity.start_offset,entity.end_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"''\", 'It', 'appears', 'to', 'be', 'a', 'very', 'daunting', 'deal', ',', \"''\", 'said', 'American', 'Technology', 'Research', 'analyst', 'Shaw', 'Wu', '.']\n",
      "American Technology Research ORG 12 15\n",
      "Shaw Wu PERSON 16 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\download\\anaconda3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "text1 = \"'' It appears to be a very daunting deal , '' said American Technology Research analyst Shaw Wu .\"\n",
    "tokens1 = text1.split(\" \")\n",
    "print(tokens1)\n",
    "doc1 = nlp(text1)\n",
    "for entity in doc1.ents:\n",
    "    print(entity.text,entity.label_,entity.start,entity.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "a = torch.rand((4,2),dtype=torch.float)\n",
    "b = torch.rand((4,2),dtype=torch.float)\n",
    "preds=[]\n",
    "preds.append(a)\n",
    "preds[0] = np.append(preds[0], b, axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "\n",
    "pred_probs = softmax(preds,axis=1)\n",
    "score_for_print = list(pred_probs[:,0])\n",
    "\n",
    "pred_label_ids = list(np.argmax(pred_probs, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2449],\n",
      "        [0.3374],\n",
      "        [0.5047],\n",
      "        [0.2891],\n",
      "        [0.1189],\n",
      "        [0.9249],\n",
      "        [0.7378],\n",
      "        [0.3168]]) tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "pred_label_ids\n",
    "labels=[1,1,0,0,0,0,1,1]\n",
    "predictions=pred_label_ids\n",
    "pred_label_ids\n",
    "probs=torch.rand((8,1))\n",
    "predictions = torch.where( probs> 0.5, 1.0, 0.0)\n",
    "print(probs,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "Metric_dict={'a':1,'b':[1,3,'a']}\n",
    "with open(\"./result/ecb+_metrics.txt\",'a',encoding='utf-8')as f:\n",
    "        json.dump(Metric_dict,f,ensure_ascii=False)\n",
    "        f.write(',\\n')\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l =torch.tensor((1,2,3))\n",
    "l.tolist()\n",
    "l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
